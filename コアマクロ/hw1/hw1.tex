\documentclass{article}
\usepackage[margin = .7in]{geometry}
\usepackage[dvipdfmx]{graphicx}
\usepackage{listings}
\usepackage{amsmath}
%\usepackage[svgnames]{xcolor}
\usepackage{bm}
\lstset{%
  language={python},
  basicstyle={\small},%
  identifierstyle={\small},%
  commentstyle={\small\itshape},%
  keywordstyle={\small\bfseries},%
  ndkeywordstyle={\small},%
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},%
  numbers=left,%
  xrightmargin=0zw,%
  xleftmargin=3zw,%
  numberstyle={\scriptsize},%
  stepnumber=1,
  numbersep=1zw,%
  lineskip=-0.5ex%
}


\begin{document}
\title{Macroeconomics 1 2018 S1S2 \\ 
Homework 1}
\author{Kei Ikegami (29186009)}
\maketitle

\section{Problem1}
 We need an additional assumption for stationarity.
 \begin{align}
 	&y_t = c + \phi y_{t-1} + \epsilon_t \nonumber \\
	\Leftrightarrow\ &(1-\phi L)y_t = c + \epsilon_t \nonumber \\
	\Leftrightarrow\ & y_t = (1-\phi L)^{-1} (c + \epsilon_t) = c + \sum_{u = 0}^{\infty} \phi^u L^u \epsilon_t = c + \sum_{u = 0}^{\infty} \phi^u \epsilon_{t-u}
 \end{align}
 For stationarity, it is necessary the above limit exists.  By Chaucy's convergence judgement, we know that it is necessary and sufficient for the convergence of series that the residual, i.e. $\sum_{u = n+1}^{\infty} \phi^u \epsilon_{t-u}$, converges to $0$. Usually, in time series analysis, the Hilbert space, whose product is defined by covariance, is used when the convergence is discussed. Thus if we want to know whether $\sum_{u = n+1}^{\infty} \phi^u \epsilon_{t-u} \to 0$ or not, we must check the convergence w.r.t the norm induced by the covariance product. And for the below calculation we need as assumption that states $Var(y_t) < \infty$. 
\begin{align*}
	\| \sum_{u = n+1}^{\infty} \phi^u \epsilon_{t-u} \| = Var(\sum_{u = n+1}^{\infty} \phi^u \epsilon_{t-u}) = \sum_{n+1}^{\infty} |\phi|^{2u}\sigma^2 = 0
\end{align*}
The second equality is followed by the additional assumption. The third equality is by the assumption $|\phi| < 1$. Now we have the result that $\sum_{u = 0}^{\infty} \phi^u \epsilon_{t-u}$ exists, in other words, the time series is stationary.

Next, I calculate the mean, variance, j-th autocovariance.
\begin{description}
	\item[Mean] By $(1)$, $E[y_t] = E\left[ c + \sum_{u = 0}^{\infty} \phi^u \epsilon_{t-u} \right] = c + 0 = c$
	\item[Variance] By $(1)$, $Var(y_t) = Var\left( \sum_{u = 0}^{\infty} \phi^u \epsilon_{t-u} \right) = \sum_{u=0}^{\infty} |\phi|^{2u} \sigma^2 = \frac{\sigma^2}{1-\phi^2}$
	\item[Autocovariance] $\gamma(t,t-j) = \phi Cov(Y_{t-1}, Y_{t-j}) = \cdots = \phi^j Var(Y_{t-j}) = \frac{\phi^u \sigma^2}{1-\phi^2}$
\end{description}

\section{Problem2}
\subsection{(1)}
\includegraphics[width = 15cm]{inflation.png}

The Python code is as follows. Note that we have to correct the log-seasonal adjustment for getting monthly inflation rates.
\lstinputlisting{hw1-2.py}

\subsection{(3)}
The R code is as follows.
\begin{lstlisting}
# read data
corrected_inflation <- read_csv("~/Desktop/2018summer/macro/hw1/empirics/corrected_inflation.csv")

# time series format
ORI <- ts(corrected_inflation[["original"]])
LOG <- ts(corrected_inflation[["log"]])
MA <- ts(corrected_inflation[["ma"]])

# ma
AIC_ma <- 1:24
for (i in 1:24){
  result <- arima(MA, order = c(i,0,0))
  AIC_ma[i] <- result$aic
}

ma_optimal_p <- which.min(AIC_ma)

# log
AIC_log <- 1:24
for (i in 1:24){
  result <- arima(LOG, order = c(i,0,0))
  AIC_log[i] <- result$aic
}

log_optimal_p <- which.min(AIC_log)
\end{lstlisting}
and $p = 13$ is chosen in both of seasonal adjustments.

\subsection{(3)}
 The estimated coefficients are in the next page. By the table, I see the stochastic significance at $ar1, ar8, ar12$ and $ar13$ in both types of seasonal adjustments. The sign is minus only for $ar12$, and the values are almost the same in the two cases. This means that the inflation rate of the same month in the previous year negatively correlates with one of this year. Then I can say the monthly inflation rate has stationary property in this data.
 \begin{table}[!htbp] \centering 
  \caption{estimating AR(13)} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lcc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & MA & LOG \\ 
\\[-1.8ex] & (1) & (2)\\ 
\hline \\[-1.8ex] 
 ar1 & 1.220$^{***}$ & 1.209$^{***}$ \\ 
  & (0.035) & (0.035) \\ 
  & & \\ 
 ar2 & $-$0.093 & $-$0.078 \\ 
  & (0.057) & (0.056) \\ 
  & & \\ 
 ar3 & $-$0.040 & $-$0.043 \\ 
  & (0.057) & (0.056) \\ 
  & & \\ 
 ar4 & $-$0.079 & $-$0.079 \\ 
  & (0.057) & (0.056) \\ 
  & & \\ 
 ar5 & 0.068 & 0.063 \\ 
  & (0.057) & (0.057) \\ 
  & & \\ 
 ar6 & $-$0.022 & $-$0.021 \\ 
  & (0.057) & (0.057) \\ 
  & & \\ 
 ar7 & $-$0.078 & $-$0.069 \\ 
  & (0.057) & (0.057) \\ 
  & & \\ 
 ar8 & 0.118$^{**}$ & 0.105$^{*}$ \\ 
  & (0.057) & (0.057) \\ 
  & & \\ 
 ar9 & 0.006 & 0.007 \\ 
  & (0.058) & (0.057) \\ 
  & & \\ 
 ar10 & $-$0.011 & $-$0.006 \\ 
  & (0.058) & (0.057) \\ 
  & & \\ 
 ar11 & $-$0.033 & $-$0.028 \\ 
  & (0.058) & (0.057) \\ 
  & & \\ 
 ar12 & $-$0.602$^{***}$ & $-$0.603$^{***}$ \\ 
  & (0.057) & (0.057) \\ 
  & & \\ 
 ar13 & 0.539$^{***}$ & 0.536$^{***}$ \\ 
  & (0.035) & (0.035) \\ 
  & & \\ 
 intercept & 0.211 & 0.203 \\ 
  & (0.150) & (0.143) \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & 564 & 564 \\ 
Log Likelihood & 1,214.527 & 1,259.025 \\ 
$\sigma^{2}$ & 0.001 & 0.001 \\ 
Akaike Inf. Crit. & $-$2,399.054 & $-$2,488.050 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 


\section{Problem3}
 I use state-space model for solving this problem. When the first difference series follow AR(2) model, i.e. $\Delta Y_t = \beta_1 \Delta Y_{t-1}+ \beta_2 \Delta Y_{t-2}$, let the state vector and the disturbance vector be
 \begin{align*}
 	X_t = \begin{pmatrix}
	\Delta Y_t \\
	\Delta Y_{t-1}
	\end{pmatrix},\ 
	e_{t} = \begin{pmatrix}
	\epsilon_{t}\\
	0
	\end{pmatrix}
 \end{align*}
 and let the update matrix be
 \begin{align*}
 	F = \begin{pmatrix}
	\beta_1 & \beta_2\\
	1 & 0
	\end{pmatrix}
 \end{align*}
then I have the below description of this model.
\begin{align*}
	X_{t+1} = F X_t + e_{t+1},\ \Delta Y_{t+1} = [1\ 0] X_{t+1}
\end{align*}

 Thus
\begin{align*}
	E_t \left[\Delta Y_{t+j}\right] &= [1\ 0 ]E_t \left[X_{t+j}\right]\\
	&= [1\ 0] E_t \left[ F X_{t+j-1} + e_{t+j} \right]\\
	&= [1\ 0] E_t \left[ F^2 X_{t+j-2} + F e_{t+j-1} + e_{t+j}\right]\\
	&= \cdots\\
	&= [1\ 0] F^j E_t \left[ X_t \right]\\
	&= [1\ 0] F^j \begin{pmatrix} \Delta Y_t\\ \Delta Y_{t-1} \end{pmatrix}
\end{align*}
and then BN trend at $t$, denoted as $\tau_t$, is derived as follows,
\begin{align*}
	\tau_t = y_t + \lim_{J \to \infty} \sum_{j = i}^J E_t\left[ \Delta Y_{t+j} \right] = y_t +  \lim_{J \to \infty} \sum_{j = i}^J [1\ 0] F^j \begin{pmatrix} \Delta Y_t\\ \Delta Y_{t-1} \end{pmatrix} = y_t + [1\ 0] \left( \sum_{j=1}^{\infty} F^j \right) \begin{pmatrix} \Delta Y_t\\ \Delta Y_{t-1} \end{pmatrix}
\end{align*}
I can calculate the limit of series of powered matrices as follows, note that $I$ is the two-dimensional identity matrix,
\begin{align*}
	 \left( \sum_{j=1}^{\infty} F^j \right) = F \left( I - F\right)^{-1} = \frac{1}{1-\beta_1 -\beta_2} \begin{pmatrix} \beta_1 + \beta_2 & \beta_2\\ 1 & \beta_2 \end{pmatrix}
\end{align*}
Then, denoting BN cycle term as $c_t$,  
\begin{align*}
	&\tau_t = y_t + \frac{1}{1-\beta_1 -\beta_2} \left\{ (\beta_1 + \beta_2)\Delta Y_t + \beta_2 \Delta Y_{t-1} \right\}\\
	&c_t = - \frac{1}{1-\beta_1 -\beta_2} \left\{ (\beta_1 + \beta_2)\Delta Y_t + \beta_2 \Delta Y_{t-1} \right\}
\end{align*}


\section{Problem4}
 All the code used in this problem is in the last of this section.
 
 \subsection{(1)}
  I plot the official GDP gap calculated by the data below.
  
 \includegraphics[width = 9cm]{official_gap.png}
 
 \subsection{(2)}
  I plot the HP cycle components.
  
  \includegraphics[width = 9cm]{hp_cycle.png}

The range of the gap shrinks by using HP filter, and the cycle is made easy to detect.

\subsection{(3)}
 I carry out the Augmented Dickey-Fuller test for the time series. And the p-value is $0.0631$, then I reject the null hypothesis. Next I make an estimation of the model for the first difference series in order to do BN-decomposition. The result says that $AR(2)$ is the best fitted model, then I decompose the time series by using the result of the problem $(3)$. And the result cycle components is plotted below.
 
   \includegraphics[width = 9cm]{BN_cycle.png}

BN-cycle show the more fluctuating property than HP-filter.
\subsection{(4)}
 First I plot the transitory component calculated by the linear regression on the trend term .
 
 \includegraphics[width = 9cm]{kinked_gap.png}
 
This seems meaningless, so I choose the best trend term form according to AIC.

 \includegraphics[width = 9cm]{kinked_gap2.png}

This looks same as the previous one.

All the code used in this problem is as foolows. And note that I had already made data easy to use for each problems before using R, then the file names listed below refer to such files done in Excel.
\begin{lstlisting}
## (1)
official_gap <- ts(official_gap[["gap"]], c(1949,1),,4)
ts.plot(official_gap)

## (2)
install.packages("mFilter")
library(mFilter)
real_gdp <- read_csv("~/Desktop/2018summer/macro/hw1/empirics/real_gdp.csv")

rgdp <- ts(real_gdp[["rgdp"]], c(1947),,4)
hp_result <- hpfilter(rgdp)
hp_trend <- hp_result$trend
hp_cycle <- hp_result$cycle
ts.plot(hp_cycle)


## (3) unit root test
install.packages("tseries")
library(tseries)
### ADF test
result_adf <- adf.test(rgdp)
### DF test
result_df <- adf.test(rgdp, k = 0)
### p-value = 0.6631, so it is not significant


## (3)BN decomposition
first_diff_rgdp <- diff(rgdp)
### find the optimal model for the first difference series.
AIC_diff <- 1:5
for (i in 1:5){
  result <- arima(first_diff_rgdp, order = c(i,0,0))
  AIC_diff[i] <- result$aic
}

diff_optimal_p <- which.min(AIC_diff)
### the chosen order is 2, so I assume the first difference series follows AR(2)

result <- arima(first_diff_rgdp, order = c(2,0,0))
beta1 <- result$coef[1]
beta2 <- result$coef[2]
BN_trend <- 1:length(rgdp)-2
for (i in 3:length(rgdp)){
  BN_trend[i-2] <- rgdp[i] + ((beta1+beta2)*first_diff_rgdp[i] + beta2*first_diff_rgdp[i-1])/(1-beta1-beta2)
}
BN_cycle <- ts(rgdp[3:length(rgdp)] - BN_trend, c(1947,7),,4)
ts.plot(BN_cycle, ylim=c(-400, 400))


## (4) Kinked trend
ind <- (1973 - 1947) * 4 + 2
# Before kink
before <- rgdp[1:ind]
# After kink
after <- rgdp[ind:length(rgdp)]
# make trend terms
before_trend <- time(before)[1:106]
after_trend <- time(after)[1:159]

# regression
before_result <- lm(before ~ before_trend)
before_gap <- residuals((before_result))
after_result <- lm(after ~ after_trend)
after_gap <- residuals((after_result))
kinked_gap <- ts(c(data.frame(before_gap)$before_gap, data.frame(after_gap)$after_gap),c(1947,1),,4)
ts.plot(kinked_gap)

# choose optimal trend term
before_trends <- list(before_trend, before_trend**2, before_trend**3, before_trend**4, before_trend**5)
after_trends <- list(after_trend, after_trend**2, after_trend**3, after_trend**4, after_trend**5)


# before
AIC_before <- 1:5
result1 <- lm(before ~ before_trend)
AIC_before[1] <- AIC(result1)
result2 <- lm(before ~ before_trend + before_trend**2)
AIC_before[2] <- AIC(result2)
result3 <- lm(before ~ before_trend + before_trend**2 + before_trend**3)
AIC_before[2] <- AIC(result3)
result4 <- lm(before ~ before_trend + before_trend**2 + before_trend**3 + before_trend**4)
AIC_before[2] <- AIC(result4)
result5 <- lm(before ~ before_trend + before_trend**2 + before_trend**3 + before_trend**4 + before_trend**5)
AIC_before[2] <- AIC(result5)

before_optimal <- which.min(AIC_before)

# after
AIC_after <- 1:5
result1 <- lm(after ~ after_trend)
AIC_after[1] <- AIC(result1)
result2 <- lm(after ~ after_trend + after_trend**2)
AIC_after[2] <- AIC(result2)
result3 <- lm(after ~ after_trend + after_trend**2 + after_trend**3)
AIC_after[2] <- AIC(result3)
result4 <- lm(after ~ after_trend + after_trend**2 + after_trend**3 + after_trend**4)
AIC_after[2] <- AIC(result4)
result5 <- lm(after ~ after_trend + after_trend**2 + after_trend**3 + after_trend**4 + after_trend**5)
AIC_after[2] <- AIC(result5)

after_optimal <- which.min(AIC_after)

## order 3 is chosen in both of periods.
trend_gap_before <- residuals(lm(before ~ before_trend + before_trend**2 + before_trend**3))
trend_gap_after <- residuals(lm(after ~ after_trend + after_trend**2 + after_trend**3))
kinked_gap2 <- ts(c(data.frame(trend_gap_before)$trend_gap_before, data.frame(trend_gap_after)$trend_gap_after),c(1947,1),,4)
ts.plot(kinked_gap2)
\end{lstlisting}

\section{Problem5}
\subsection{(1)}
\begin{align}
	\begin{cases}
	y_t = a_{10} + a_{11}y_{t-1} + a_{12}z_{t-1} + e_{1t}\\
	z_t = a_{20} + a_{21}y_{t-1} + a_{22}z_{t-1} + e_{2t}
	\end{cases}
\end{align}
By $(2)$, I get the below,
\begin{align}
	\begin{cases}
	(1-a_{11}L)y_t = a_{10} + a_{12}z_{t-1} + e_{1t}\\
	(1-a_{22}L)z_t = a_{20} + a_{21}y_{t-1} + e_{2t}
	\end{cases}
	\Rightarrow\ 
	\begin{cases}
	y_t = (1-a_{11}L)^{-1} \left( a_{10} + a_{12}z_{t-1} + e_{1t} \right)\\
	z_t = (1-a_{22}L)^{-1} \left( a_{20} + a_{21}y_{t-1} + e_{2t} \right)
	\end{cases}
\end{align}
Thus I have the following AR(2) form for each time series,
\begin{align}
	\begin{cases}
	y_t = a_{10} + a_{11}y_{t-1} + a_{12} (1-a_{22}L)^{-1} \left( a_{20} + a_{21}y_{t-2} + e_{2t} \right) + e_{1t}\\
	z_t = a_{20} + a_{21} (1-a_{11}L)^{-1} \left( a_{10} + a_{12}z_{t-2} + e_{1t} \right) + a_{22}z_{t-1} + e_{2t}
	\end{cases}
\end{align}

\subsection{(2)}
 Because of symmetry, I show the procedure only for $y_t$ and just show the result w.r.t $z_t$. By $(2),(3)$, I have the following representation.
\begin{align*}
	y_t = &(1-a_{11}L)^{-1}\left\{ a_{10}+(1-a_{22}L)^{-1} a_{12}a_{20} \right\} \\
	+ &a_{12}a_{21}(1-a_{11}L)^{-1}(1-a_{22}L)^{-1}y_{t-2} \\
	+ &a_{12}(1-a_{11}{L})^{-1}(1-a_{22}L)^{-1}e_{2.t-1}\\
	+ &(1-a_{11}L)^{-1}e_{1,t}
\end{align*}
Then I have the following explicit form. Note that $\lambda_1, \lambda_2$ are the inverse of the solutions of $1-(a_{11} + a_{22})x + (a_{11}a_{22} - a_{12}a_{21})x^2$, and I assume that this equation has the real number solution, in other words $(a_{11} +a_{22})^2 -4(a_{11}a_{22} - a_{12}a_{21}) = (a_{11} -aa_{22})^2 + 4a_{12}a_{21} \geq 0$.
\begin{align*}
	y_t = &\left( 1 - a_{12}a_{21} (1-a_{11}L)^{-1}(1-a_{22}L)^{-1} L^2\right)^{-1} \\
	&\left[(1-a_{11}L)^{-1} \left\{ a_{10}+(1-a_{22}L)^{-1} a_{12}a_{20} \right\}
	+ a_{12}(1-a_{11}{L})^{-1}(1-a_{22}L)^{-1}e_{2.t-1} + (1-a_{11}L)^{-1}e_{1,t} \right]\\[8pt]
	= &\left( 1 - \frac{a_{12}a_{21}L^2}{(1-a_{11}L)(1-a_{22}L)}\right)^{-1}\\
	&\left[(1-a_{11}L)^{-1} \left\{ a_{10}+(1-a_{22}L)^{-1} a_{12}a_{20} \right\}
	+ a_{12}(1-a_{11}{L})^{-1}(1-a_{22}L)^{-1}e_{2.t-1} + (1-a_{11}L)^{-1}e_{1,t} \right]\\[8pt]
	=&\left( 1-(a_{11} + a_{22})L + (a_{11}a_{22} - a_{12}a_{21})L^2 \right)^{-1} \left( 1-a_{11}L \right) \left( 1-a_{22}L \right)\\
	&\left[(1-a_{11}L)^{-1} \left\{ a_{10}+(1-a_{22}L)^{-1} a_{12}a_{20} \right\}
	+ a_{12}(1-a_{11}{L})^{-1}(1-a_{22}L)^{-1}e_{2.t-1} + (1-a_{11}L)^{-1}e_{1,t} \right]\\[8pt]
	=& (1- \lambda_1 L)^{-1} (1- \lambda_2 L)^{-1} \left( 1-a_{11}L \right) \left( 1-a_{22}L \right)\\
	&\left[(1-a_{11}L)^{-1} \left\{ a_{10}+(1-a_{22}L)^{-1} a_{12}a_{20} \right\}
	+ a_{12}(1-a_{11}{L})^{-1}(1-a_{22}L)^{-1}e_{2.t-1} + (1-a_{11}L)^{-1}e_{1,t} \right]\\[8pt]
	= & (1- \lambda_1 L)^{-1} (1- \lambda_2 L)^{-1} \left\{ a_{10} - a_{22} a_{10} + a_{12} a_{20} \right\}\\
	&+ a_{12} \left( 1-\lambda_1 L \right)^{-1} (1- \lambda_2 L)^{-1} e_{2,t-1}\\
	&+ (1- \lambda_1 L)^{-1} (1- \lambda_2 L)^{-1} \left( e_{1,t} - a_{22}e_{1,t-1} \right)\\[8pt]
	=&  \left\{ a_{10} - a_{22} a_{10} + a_{12} a_{20} \right\} \left( \sum_{j = 0}^{\infty} \sum_{k = 0}^{j} \lambda_1^k \lambda_2^{j-k} \right)\\
	&+a_{12} \sum_{j = 0}^{\infty}\left( \sum_{k = 0}^j \lambda_1^k \lambda_2^{j-k}\right) e_{2, t-1-j}\\
	&+  \sum_{j = 0}^{\infty} \left( \sum_{k = 0}^j \lambda_1^k \lambda_2^{j-k}\right) e_{1, t-j } - a_{22}  \sum_{j = 0}^{\infty} \left( \sum_{k = 0}^j \lambda_1^k \lambda_2^{j-k}\right) e_{1, t-1-j }
\end{align*}

The same calculation can be applied to $\left\{ z_t \right\}$.
\begin{align*}
	z_t =& \left\{ a_{20} - a_{11} a_{20} + a_{21} a_{10} \right\} \left( \sum_{j = 0}^{\infty} \sum_{k = 0}^{j} \lambda_1^k \lambda_2^{j-k} \right)\\
	&+a_{21} \sum_{j = 0}^{\infty}\left( \sum_{k = 0}^j \lambda_1^k \lambda_2^{j-k}\right) e_{1, t-1-j}\\
	&+  \sum_{j = 0}^{\infty} \left( \sum_{k = 0}^j \lambda_1^k \lambda_2^{j-k}\right) e_{2, t-j } - a_{11}  \sum_{j = 0}^{\infty} \left( \sum_{k = 0}^j \lambda_1^k \lambda_2^{j-k}\right) e_{2, t-1-j }
\end{align*}

\subsection{(3)}
 Now I have the explicit representation as follows, since in this case $\lambda_1, \lambda_2 = \frac{3}{5}, 1$.
\begin{align*}
	y_t &= 0.2 \sum_{j = 0}^{\infty} \left( \sum_{k = 0}^{j} \frac{3}{5}^{k} \right) e_{2, t-1-j}+\sum_{j = 0}^{\infty} \left( \sum_{k = 0}^{j} \frac{3}{5}^{k} \right) e_{1, t-j} + 0.8 \sum_{j = 0}^{\infty} \left( \sum_{k = 0}^{j} \frac{3}{5}^{k} \right) e_{1, t-1-j}\\[10pt]
	&= 0.5 \sum_{j = 0}^{\infty} \left( 1 - \frac{3}{5}^j\right) e_{2, t-1-j} + 2.5 \sum_{j = 0}^{\infty} \left( 1 - \frac{3}{5}^j\right) e_{1, t-j} + 2 \sum_{j = 0}^{\infty} \left( 1 - \frac{3}{5}^j\right) e_{1, t-1-j}
\end{align*}
Now each term in the above does not exist, which means all the term diverge, so this is not a stationary process. And the standard stationarity judgement for VAR model is that all the eigenvalue of $\begin{pmatrix} a_{11} & a_{12}\\a_{21} & a_{22} \end{pmatrix}$ are less than $1$. The current parameter values give,
\begin{align*}
	\left| \begin{pmatrix} 0.8-\lambda & 0.2\\0.2 & 0.8-\lambda \end{pmatrix} \right| = 0\ \Leftrightarrow \lambda = 1, 0.6
\end{align*}
Then again I have non-stationarity.

\subsection{(4)}
\begin{align*}
 &\frac{\partial y_{t+s}}{\partial e_{1, t}} = \sum_{k = 0}^{s} \frac{3}{5}^k - 0.8 \sum_{k = 0}^{s-1} \frac{3}{5}^k = \frac{1}{2} + \frac{1}{2} \left( \frac{3}{5} \right)^{s-1}\\[8pt]
 &\frac{\partial y_{t+s}}{\partial e_{2, t}} = 0.2 \sum_{k = 0}^{s-1} \frac{3}{5}^k = \frac{1}{2} -\frac{1}{2} \left( \frac{3}{5}\right)^{s-1}
\end{align*}

\subsection{(5)}
\begin{align*}
	&\frac{\partial y_{t+s}}{\partial \epsilon_{y, t}} = \frac{\partial e_{1,t}}{\partial \epsilon_{y, t}} \frac{\partial y_{t+s}}{\partial e_{1, t}} + \frac{\partial e_{2,t}}{\partial \epsilon_{y, t}} \frac{\partial y_{t+s}}{\partial e_{2, t}} = \frac{\partial y_{t+s}}{\partial e_{1, t}} = \frac{1}{2} + \frac{1}{2} \left( \frac{3}{5} \right)^{s-1}\\[8pt]
	&\frac{\partial y_{t+s}}{\partial \epsilon_{z, t}} = \frac{\partial e_{1,t}}{\partial \epsilon_{z, t}} \frac{\partial y_{t+s}}{\partial e_{1, t}} + \frac{\partial e_{2,t}}{\partial \epsilon_{z, t}} \frac{\partial y_{t+s}}{\partial e_{2, t}} = \frac{1}{2}\left(  \frac{1}{2} + \frac{1}{2} \left( \frac{3}{5} \right)^{s-1}\right) + \frac{1}{2} -\frac{1}{2} \left( \frac{3}{5}\right)^{s-1} = \frac{3}{4} - \frac{1}{4} \left( \frac{3}{5} \right)^{s-1}
\end{align*}

\subsection{(6)}
\begin{align*}
	&\frac{\partial y_{t+s}}{\partial \epsilon_{y, t}} = \frac{\partial e_{1,t}}{\partial \epsilon_{y, t}} \frac{\partial y_{t+s}}{\partial e_{1, t}} + \frac{\partial e_{2,t}}{\partial \epsilon_{y, t}} \frac{\partial y_{t+s}}{\partial e_{2, t}} = \frac{\partial y_{t+s}}{\partial e_{1, t}} + \frac{1}{2} \frac{\partial y_{t+s}}{\partial e_{2, t}}= \frac{1}{2} + \frac{1}{2} \left( \frac{3}{5} \right)^{s-1} + \frac{1}{2} \left( \frac{1}{2} -\frac{1}{2} \left( \frac{3}{5}\right)^{s-1} \right) = \frac{3}{4}+\frac{1}{4}\left( \frac{3}{5} \right)^{s-1} \\[8pt]
	&\frac{\partial y_{t+s}}{\partial \epsilon_{z, t}} = \frac{\partial e_{1,t}}{\partial \epsilon_{z, t}} \frac{\partial y_{t+s}}{\partial e_{1, t}} + \frac{\partial e_{2,t}}{\partial \epsilon_{z, t}} \frac{\partial y_{t+s}}{\partial e_{2, t}} = \frac{1}{2} -\frac{1}{2} \left( \frac{3}{5}\right)^{s-1} 
\end{align*}


\subsection{(7)}
By problem (5),(6), I get to know the error term composition has an impact on IRF. In problem(5), $\epsilon_{y,t}$ has no influence on $e_{2, t}$, which means that the shock to equation (1) is external to equation (6), in other words the unique shock to $y_t$ is assumed to be exogenous to $z_t$. In problem (6), the reverse condition is assumed. Thus, when we analyze real data set by using VAR, we should be careful about the stacking order of all the variables so that the former variable is exogenous to all the later ones. 
\end{document}
























