\documentclass{article}
\usepackage[margin = .7in]{geometry}
\usepackage{multirow,array}
\usepackage[dvipdfmx]{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{bm}

\begin{document}
\title{STAT3811/3955\ Survival\ Analysis\\ Assignment 1}
\author{Kei Ikegami}
\maketitle

\section{Q1}
\subsection{(a)}
\begin{align*}
	E[T | T > t] &= \int_{t}^{\infty} T f(T|T > t) \mathrm{d}T = \int_{t}^{\infty} T \frac{f(T)}{1-F(t)} \mathrm{d}T\\
	&= \frac{1}{1 - F(t)} \left( \int_{0}^{\infty} T f(T) \mathrm{d}T - \int_0^t T f(T) \mathrm{d}T \right) = \frac{1}{1 - F(t)} \left( \mu - t F(t) + \int_0^t F(T) \mathrm{d}T \right) \\
\end{align*}
So I get the below derivative of $m(t)$.
\begin{align*}
	m^{'}(t) = \frac{1}{(1 - F(t))^2} \left\{ \left( -F(t) - tf(t) + F(t) \right)(1 - F(t)) + (\mu - tF(t) + \int_0^t F(T) \mathrm{d}T) f(t) \right\} -1
\end{align*} 
Then the following calculation leads to the result.
\begin{align*}
	\frac{1 + m^{'}(t)}{m(t)} = \frac{f(t)\left(\mu - tF(t) + \int_0^t F(T) \mathrm{d}T - t(1- F(t)) \right)}{(1-F(t)) \left(\mu - tF(t) + \int_0^t F(T) \mathrm{d}T \right) - t(1 - F(t))^2} = \frac{f(t)}{1-F(t)} = \lambda(t)
\end{align*}

\subsection{(b)}
Since $\int_0^t F(T) \mathrm{d}T = \int_0^t (-S(T) + 1) \mathrm{d}T = -\int_0^t S(T) \mathrm{d}T + t$, then
\begin{align*}
	m(t) &= \frac{1}{S(t)} \int_0^{\infty} (T - t) f(T) \mathrm{d}T = \frac{1}{S(t)} \left( \int_{0}^{\infty} (T-t) f(T) \mathrm{d}T - \int_0^t (T-t) f(T) \mathrm{d}T \right)\\
	&= \frac{1}{S(t)} \left( \mu - t + \int_0^t F(T) \mathrm{d}T \right) = \frac{1}{S(t)} \left( \mu - \int_0^t S(u) \mathrm{d}u \right)
\end{align*}
Now, when $T$ has an exponential distribution with $\mu = \frac{1}{\lambda}$,
\begin{align*}
	m(t) = \exp(\lambda t) \left( \mu + \frac{1}{\lambda} \exp(-\lambda t) - \frac{1}{\lambda} \right) = \frac{1}{\lambda} = \mu
\end{align*}
because $\int_0^{\infty} t \lambda \exp(-\lambda t) \mathrm{d} t = \frac{1}{\lambda}$.

\subsection{(c)}
First I consider the mean,
\begin{align*}
	\lim_{t \to 0} m(t) = \lim_{t \to 0} E[T | T > t] = E[T] = 1
\end{align*}
Now let $\delta = med(T)$, then $F(\delta) = \frac{1}{2}$ and $\lambda(\delta) = \frac{2}{\delta + 1}$ due to (a). Then by using (b) I get the below calculation.
\begin{align*}
	\frac{2}{\delta + 1} = 2 \left( 1 - \int_0^{\delta} (1 - F(u))\mathrm{d}u \right)
	\Leftrightarrow \int_0^{\delta} (1 - F(u)) \mathrm{d}u = \frac{\delta}{\delta + 1}
\end{align*}
By taking derivative of both sides about $\delta$, I get the result as follows.
\begin{align*}
	1 - F(\delta) = \frac{1}{(\delta+1)^2} \quad \Leftrightarrow \quad \frac{1}{2} = \frac{1}{(\delta+1)^2} \quad \Leftrightarrow \quad \delta = \sqrt{2} -1
\end{align*}

\subsection{(d)}
First I have the below representation of $m(t)$.
\begin{align*}
	m(t) \frac{\mu - \int_0^t S(u) \mathrm{d}u}{S(t)} = \frac{\int_t^{\infty} S(u) \mathrm{d}u}{S(t)}
\end{align*}
Since the limits of the both of enumerator and denominator are 0 as $t \to \infty$. By using L'Hopital's rule twice, I get the below result,
\begin{align*}
	\lim_{t \to \infty} m(t) = \lim_{t \to \infty} \frac{-S(t)}{-f(t)} = \lim_{t \to \infty} \frac{f(t)}{-f^{'}(t)} = \lim_{t \to \infty} \left( -\frac{\mathrm{d}}{\mathrm{d}t} {\rm log} f(t) \right)^{-1}
\end{align*}

\subsection{(e)}
In this case, $f(t) = \frac{1}{\sqrt{2\pi} \sigma t} \exp(\frac{{\rm log} t - \mu}{2\sigma^2})$, I use (d) to get the result.
\begin{align*}
	\left( \frac{\mathrm{d}}{\mathrm{d}t} {\rm log} f(t) \right)^{-1} = -\frac{f(t)}{f^{'}(t)} = - \frac{\sigma^2 t}{\mu - {\rm log}t - \sigma^2}\\
	\lim_{t \to \infty} -\frac{\sigma^2 t}{\mu - {\rm log}t - \sigma^2} = \lim_{t \to \infty} -\frac{1}{-\frac{1}{t}} = \infty
\end{align*}

\section{Q3}
\subsection{(a)}
By definition, $S(t|z) = 1 - F(t|z)$. Thus I calculate $F(t|z)$ as follows.
\begin{align*}
	F(t|z) = {\rm Pr}(Y \leq {\rm log}t |z) = {\rm Pr}(w \leq \frac{{\rm log}\ t - \mu- \beta z}{\sigma} |z)
\end{align*}
Because $\int_{-\infty}^{\omega} \frac{\exp(u)}{(1 + \exp(u))^2} \mathrm{d}u = \frac{\exp(\omega)}{1 + \exp(\omega)}$, then by the above calculation,
\begin{align*}
	S(t|z) = 1- F(t|z) = \frac{1}{1 + \exp(\frac{{\rm log}\ t - \mu- \beta z}{\sigma})}
\end{align*}

\subsection{(b)}
By (a),
\begin{align*}
	\frac{S(t|z)}{1 - S(t|z)} = \frac{1}{\exp(\frac{{\rm log}\ t - \mu- \beta z}{\sigma})} = \exp \left(- \frac{{\rm log}\ t - \mu- \beta z}{\sigma} \right)
\end{align*}

\subsection{(c)}
By (b), let $Odds_i$ be the odds for $z_i$,
\begin{align*}
	\frac{Odds_1}{Odds_2} = \exp \left(\frac{\beta}{\sigma} \right)
\end{align*}
And this odds ratio is independent of $t$.

\section{Q5}
\subsection{(a)}
Just calculate as follows,
\begin{align*}
	P(T_i < C_i) &= \int_0^{\infty} \left( \int_0^c \lambda \exp\left( -\lambda t \right) \mathrm{d}t\right) \theta \exp \left( -\theta c \right) \mathrm{d}c = 1 - \int_0^{\infty} \theta \exp \left( -(\lambda+\theta)c \right) \mathrm{d}c \\
	&= 1 - \frac{\theta}{\theta + \lambda} = \frac{\lambda}{ \theta + \lambda}
\end{align*}
Then the probability distribution of $\delta$ is
\begin{equation}
	\delta = 
	\begin{cases}
		1 & \text{with probability $\frac{\lambda}{\lambda + \theta}$} \\
		0 & \text{with probability $\frac{\theta}{\lambda + \theta}$} \\
		\text{otherwise} & \text{with probability $0$}
	\end{cases} \nonumber
\end{equation}

\subsection{(b)}
Let $F_Y(y),\ f_Y(y)$ be the distribution function and probability distribution function of $Y$. Then, due to the independence of $T$ and $C$,
\begin{align*}
	1 - F_Y(y) = 1- {\rm Pr} (\min(T, C) < y) = {\rm Pr}(y \leq \min(T,C)) = {\rm Pr}(y \leq T) {\rm Pr}(y \leq C) = \exp(-(\lambda + \theta)y)
\end{align*}
Thus I get $F_Y(y) = 1 - \exp(-(\lambda + \theta)y)$, which means $Y$ has a exponential distribution with parameter $\lambda + \theta$.

\subsection{(c)}
Consider the marginal distribution of $Y$ when $\delta = 1$ as follows,
\begin{align*}
	f(Y, \delta = 1) = \lim_{h \to 0} \frac{{\rm Pr}(y \leq Y \leq y + h, \delta = 1)}{h}
\end{align*}
Now the denominator of this can be decomposed, because of the independence of $T, C$, 
\begin{align*}
	{\rm Pr}(y \leq Y \leq y + h, \delta = 1) &= {\rm Pr}(y \leq Y \leq y + h, T < C) = {\rm Pr}(y \leq T \leq y + h, y \leq C) \\
	&= {\rm Pr}(y \leq T \leq y + h) {\rm Pr}(y \leq C) = \exp(-\lambda y) (1 - \exp(-\lambda h)) \exp(-\theta y) \\
\end{align*}
Then, by using L'Hopital rule, the marginal distribution is 
\begin{align*}
	f(Y, \delta = 1) &= \exp(-(\lambda+\theta)y) \lim_{h \to 0} \frac{1 - \exp(-\lambda h)}{h}\\
	&= \lambda \exp(-(\lambda+\theta)y) = \left(\frac{\lambda}{\lambda+ \theta}\right) (\lambda+\theta)\exp(-(\lambda+\theta)y)
\end{align*}
The same is true of $\delta = 0$ case, so the joint probability distribution function is expressed as the multiplication of random variable's probability distribution function. This means that the two random variables are independent from each pther.

\subsection{(d)}
Consider the distribution function of $W_2$, denote its distribution function as $F_W(w)$,
\begin{align*}
	F_W(w) &= {\rm Pr}(T_1 + T_2 < w) = {\rm Pr}(T_1 < w - T_2)\\
	&= \int_0^w \left( \int_0^{w - t_2} \lambda \exp(-\lambda t_1) \mathrm{d}t_1 \right) \lambda \exp(-\lambda t_2) \mathrm{d}t_2
	=(1 - \exp(-\lambda w)) - \int_0^w \lambda \exp(-\lambda w) \mathrm{d}t_2 \\
	&= 1 - \exp(-\lambda w) - w \lambda \exp(-\lambda w)
\end{align*}
Then, by taking derivative of the above, I get the pdf $f(w) = \lambda^2 w \exp(-\lambda w)$.

\subsection{(e)}
\subsection{(f)}
\subsection{(g)}
\subsection{(h)}
\subsection{(i)}
\subsection{(j)}

\end{document}






































