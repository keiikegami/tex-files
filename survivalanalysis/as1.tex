\documentclass{article}
\usepackage[margin = .7in]{geometry}
\usepackage{multirow,array}
\usepackage[dvipdfmx]{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{bm}

\begin{document}
\title{STAT3811/3955\ Survival\ Analysis\\ Assignment 1}
\author{Kei Ikegami}
\maketitle

\section{Q1}
\subsection{(a)}
\begin{align*}
	E[T | T > t] &= \int_{t}^{\infty} T f(T|T > t) \mathrm{d}T = \int_{t}^{\infty} T \frac{f(T)}{1-F(t)} \mathrm{d}T\\
	&= \frac{1}{1 - F(t)} \left( \int_{0}^{\infty} T f(T) \mathrm{d}T - \int_0^t T f(T) \mathrm{d}T \right) = \frac{1}{1 - F(t)} \left( \mu - t F(t) + \int_0^t F(T) \mathrm{d}T \right) \\
\end{align*}
So I get the below derivative of $m(t)$.
\begin{align*}
	m^{'}(t) = \frac{1}{(1 - F(t))^2} \left\{ \left( -F(t) - tf(t) + F(t) \right)(1 - F(t)) + (\mu - tF(t) + \int_0^t F(T) \mathrm{d}T) f(t) \right\} -1
\end{align*} 
Then the following calculation leads to the result.
\begin{align*}
	\frac{1 + m^{'}(t)}{m(t)} = \frac{f(t)\left(\mu - tF(t) + \int_0^t F(T) \mathrm{d}T - t(1- F(t)) \right)}{(1-F(t)) \left(\mu - tF(t) + \int_0^t F(T) \mathrm{d}T \right) - t(1 - F(t))^2} = \frac{f(t)}{1-F(t)} = \lambda(t)
\end{align*}

\subsection{(b)}
Since $\int_0^t F(T) \mathrm{d}T = \int_0^t (-S(T) + 1) \mathrm{d}T = -\int_0^t S(T) \mathrm{d}T + t$, then
\begin{align*}
	m(t) &= \frac{1}{S(t)} \int_0^{\infty} (T - t) f(T) \mathrm{d}T = \frac{1}{S(t)} \left( \int_{0}^{\infty} (T-t) f(T) \mathrm{d}T - \int_0^t (T-t) f(T) \mathrm{d}T \right)\\
	&= \frac{1}{S(t)} \left( \mu - t + \int_0^t F(T) \mathrm{d}T \right) = \frac{1}{S(t)} \left( \mu - \int_0^t S(u) \mathrm{d}u \right)
\end{align*}
Now, when $T$ has an exponential distribution with $\mu = \frac{1}{\lambda}$,
\begin{align*}
	m(t) = \exp(\lambda t) \left( \mu + \frac{1}{\lambda} \exp(-\lambda t) - \frac{1}{\lambda} \right) = \frac{1}{\lambda} = \mu
\end{align*}
because $\int_0^{\infty} t \lambda \exp(-\lambda t) \mathrm{d} t = \frac{1}{\lambda}$.

\subsection{(c)}
First I consider the mean,
\begin{align*}
	\lim_{t \to 0} m(t) = \lim_{t \to 0} E[T | T > t] = E[T] = 1
\end{align*}
Now let $\delta = med(T)$, then $F(\delta) = \frac{1}{2}$ and $\lambda(\delta) = \frac{2}{\delta + 1}$ due to (a). Then by using (b) I get the below calculation.
\begin{align*}
	\frac{2}{\delta + 1} = 2 \left( 1 - \int_0^{\delta} (1 - F(u))\mathrm{d}u \right)
	\Leftrightarrow \int_0^{\delta} (1 - F(u)) \mathrm{d}u = \frac{\delta}{\delta + 1}
\end{align*}
By taking derivative of both sides about $\delta$, I get the result as follows.
\begin{align*}
	1 - F(\delta) = \frac{1}{(\delta+1)^2} \quad \Leftrightarrow \quad \frac{1}{2} = \frac{1}{(\delta+1)^2} \quad \Leftrightarrow \quad \delta = \sqrt{2} -1
\end{align*}

\subsection{(d)}
First I have the below representation of $m(t)$.
\begin{align*}
	m(t) \frac{\mu - \int_0^t S(u) \mathrm{d}u}{S(t)} = \frac{\int_t^{\infty} S(u) \mathrm{d}u}{S(t)}
\end{align*}
Since the limits of the both of enumerator and denominator are 0 as $t \to \infty$. By using L'Hopital's rule twice, I get the below result,
\begin{align*}
	\lim_{t \to \infty} m(t) = \lim_{t \to \infty} \frac{-S(t)}{-f(t)} = \lim_{t \to \infty} \frac{f(t)}{-f^{'}(t)} = \lim_{t \to \infty} \left( -\frac{\mathrm{d}}{\mathrm{d}t} {\rm log} f(t) \right)^{-1}
\end{align*}

\subsection{(e)}
In this case, $f(t) = \frac{1}{\sqrt{2\pi} \sigma t} \exp(\frac{{\rm log} t - \mu}{2\sigma^2})$, I use (d) to get the result.
\begin{align*}
	\left( \frac{\mathrm{d}}{\mathrm{d}t} {\rm log} f(t) \right)^{-1} = -\frac{f(t)}{f^{'}(t)} = - \frac{\sigma^2 t}{\mu - {\rm log}t - \sigma^2}\\
	\lim_{t \to \infty} -\frac{\sigma^2 t}{\mu - {\rm log}t - \sigma^2} = \lim_{t \to \infty} -\frac{1}{-\frac{1}{t}} = \infty
\end{align*}

\section{Q3}
\subsection{(a)}
By definition, $S(t|z) = 1 - F(t|z)$. Thus I calculate $F(t|z)$ as follows.
\begin{align*}
	F(t|z) = {\rm Pr}(Y \leq {\rm log}t |z) = {\rm Pr}(w \leq \frac{{\rm log}\ t - \mu- \beta z}{\sigma} |z)
\end{align*}
Because $\int_{-\infty}^{\omega} \frac{\exp(u)}{(1 + \exp(u))^2} \mathrm{d}u = \frac{\exp(\omega)}{1 + \exp(\omega)}$, then by the above calculation,
\begin{align*}
	S(t|z) = 1- F(t|z) = \frac{1}{1 + \exp(\frac{{\rm log}\ t - \mu- \beta z}{\sigma})}
\end{align*}

\subsection{(b)}
By (a),
\begin{align*}
	\frac{S(t|z)}{1 - S(t|z)} = \frac{1}{\exp(\frac{{\rm log}\ t - \mu- \beta z}{\sigma})} = \exp \left(- \frac{{\rm log}\ t - \mu- \beta z}{\sigma} \right)
\end{align*}

\subsection{(c)}
By (b), let $Odds_i$ be the odds for $z_i$,
\begin{align*}
	\frac{Odds_1}{Odds_2} = \exp \left(\frac{\beta}{\sigma} \right)
\end{align*}
And this odds ratio is independent of $t$.

\section{Q5}
\subsection{(a)}
Just calculate as follows,
\begin{align*}
	P(T_i < C_i) &= \int_0^{\infty} \left( \int_0^c \lambda \exp\left( -\lambda t \right) \mathrm{d}t\right) \theta \exp \left( -\theta c \right) \mathrm{d}c = 1 - \int_0^{\infty} \theta \exp \left( -(\lambda+\theta)c \right) \mathrm{d}c \\
	&= 1 - \frac{\theta}{\theta + \lambda} = \frac{\lambda}{ \theta + \lambda}
\end{align*}
Then the probability distribution of $\delta$ is
\begin{equation}
	\delta = 
	\begin{cases}
		1 & \text{with probability $\frac{\lambda}{\lambda + \theta}$} \\
		0 & \text{with probability $\frac{\theta}{\lambda + \theta}$} \\
		\text{otherwise} & \text{with probability $0$}
	\end{cases} \nonumber
\end{equation}

\subsection{(b)}
Let $F_Y(y),\ f_Y(y)$ be the distribution function and probability distribution function of $Y$. Then, due to the independence of $T$ and $C$,
\begin{align*}
	1 - F_Y(y) = 1- {\rm Pr} (\min(T, C) < y) = {\rm Pr}(y \leq \min(T,C)) = {\rm Pr}(y \leq T) {\rm Pr}(y \leq C) = \exp(-(\lambda + \theta)y)
\end{align*}
Thus I get $F_Y(y) = 1 - \exp(-(\lambda + \theta)y)$, which means $Y$ has a exponential distribution with parameter $\lambda + \theta$.

\subsection{(c)}
Consider the marginal distribution of $Y$ when $\delta = 1$ as follows,
\begin{align*}
	f(Y, \delta = 1) = \lim_{h \to 0} \frac{{\rm Pr}(y \leq Y \leq y + h, \delta = 1)}{h}
\end{align*}
Now the denominator of this can be decomposed, because of the independence of $T, C$, 
\begin{align*}
	{\rm Pr}(y \leq Y \leq y + h, \delta = 1) &= {\rm Pr}(y \leq Y \leq y + h, T < C) = {\rm Pr}(y \leq T \leq y + h, y \leq C) \\
	&= {\rm Pr}(y \leq T \leq y + h) {\rm Pr}(y \leq C) = \exp(-\lambda y) (1 - \exp(-\lambda h)) \exp(-\theta y) \\
\end{align*}
Then, by using L'Hopital rule, the marginal distribution is 
\begin{align*}
	f(Y, \delta = 1) &= \exp(-(\lambda+\theta)y) \lim_{h \to 0} \frac{1 - \exp(-\lambda h)}{h}\\
	&= \lambda \exp(-(\lambda+\theta)y) = \left(\frac{\lambda}{\lambda+ \theta}\right) (\lambda+\theta)\exp(-(\lambda+\theta)y)
\end{align*}
The same is true of $\delta = 0$ case, so the joint probability distribution function is expressed as the multiplication of random variable's probability distribution function. This means that the two random variables are independent from each pther.

\subsection{(d)}
Consider the distribution function of $W_2$, denote its distribution function as $F_W(w)$,
\begin{align*}
	F_W(w) &= {\rm Pr}(T_1 + T_2 < w) = {\rm Pr}(T_1 < w - T_2)\\
	&= \int_0^w \left( \int_0^{w - t_2} \lambda \exp(-\lambda t_1) \mathrm{d}t_1 \right) \lambda \exp(-\lambda t_2) \mathrm{d}t_2
	=(1 - \exp(-\lambda w)) - \int_0^w \lambda \exp(-\lambda w) \mathrm{d}t_2 \\
	&= 1 - \exp(-\lambda w) - w \lambda \exp(-\lambda w)
\end{align*}
Then, by taking derivative of the above, I get the pdf $f(w) = \lambda^2 w \exp(-\lambda w)$.
And the same way shows that $W_m$ has an gamma distribution with the parameter $m, \lambda$. i.e.
\begin{align*}
	f(w_m) =\frac{\lambda^m (w_m)^{m-1} \exp(-\lambda w_m)}{\Gamma(m)}
\end{align*}

\subsection{(e)}
Let $L$ be the likelihood, then
\begin{align*}
	L = \Pi_{i=1}^n \left[ (\lambda \exp(-\lambda y_i))^{\delta_i} (\exp(-\lambda y_i))^{1-\delta_i} \right] \left[ (\theta \exp(-\theta y_i))^{\delta_i} (\exp(-\theta y_i))^{1-\delta_i} \right]
\end{align*}
Let $l$ be the loglikelihood, then
\begin{align*}
	l &= \sum_{i=1}^n \delta_i ({\rm log}\ \lambda - \lambda y_i - \theta y_i) + (1 - \delta_i) ({\rm log}\ \theta - \lambda y_i - \theta y_i)\\
	&= n {\rm log}\ \theta - (\lambda+\theta) \sum_{i=1}^n y_i + {\rm log} \frac{\lambda}{\theta} \sum_{i=1}^n \delta_i
\end{align*}
Thus the MLE is obtained as follows.
\begin{align*}
	\quad \frac{\partial l}{\partial \lambda} = 0 &\quad \Leftrightarrow \quad -\sum_{i=1}^n y_i + \frac{\theta}{\lambda} \frac{1}{\theta} \sum_{i=1}^n \delta_i = 0 \\[10pt]
	&\quad \Leftrightarrow \quad \lambda = \frac{\sum_{i=1}^n \delta_i}{\sum_{i=1}^n y_i}
\end{align*}

\subsection{(f)}
\begin{align*}
	E[\hat{\lambda}] = E\left[ \frac{\sum_{i=1}^n \delta_i }{\sum_{i=1}^n y_i} \right] = E\left[\frac{1}{\sum_{i=1}^n y_i} \right]\ (E[\delta_1] + E[\delta_2] + \dots + E[\delta_n]) = E\left[\frac{1}{\sum_{i=1}^n y_i} \right] \frac{n\lambda}{\lambda + \theta}
\end{align*}
The last equality is from that $E[\delta_i] = \frac{\lambda}{\lambda+\theta}$ for all $i$. Now, from (d), the last expectation can be calculated as follows. (let $z$ represent the summation of $y$)
\begin{align*}
	E\left[\frac{1}{\sum_{i=1}^n y_i} \right] &= \int_0^{\infty} \frac{1}{z} \frac{(\lambda+\theta)^n z^{n-1} \exp(-(\lambda+\theta)z)}{\Gamma(n)} \mathrm{d}z\\[10pt]
	&= \frac{\Gamma(n-1)}{\Gamma(n)} (\lambda+\theta) \int_0^{\infty} \frac{(\lambda+\theta)^{n-1} z^{(n-1) -1} \exp(-(\lambda+\theta)z)}{\Gamma(n-1)} \mathrm{d}z\\[10pt]
	&= \frac{\lambda+\theta}{n-1}
\end{align*}
By the above computation, we get $E[\hat{\lambda}] = \frac{\lambda+\theta}{n-1} \frac{n\lambda}{\lambda+\theta} = \frac{n\lambda}{n-1}$. And the limit is clearly $\lambda$.

\subsection{(g)}
By using (f)'s notation, and from the previous results, I get 
\begin{align*}
	&Var(\delta_1) = \frac{\lambda\theta}{(\lambda+\theta)^2}\\[10pt]
	&E[(\sum_{i=1}^n \delta_i)^2] = Var(\sum_{i=1}^n \delta_i) + (E[\sum_{i=1}^n \delta_i])^2 = n\ Var(\delta_1) + (\frac{n\lambda}{\lambda+\theta})^2 = \frac{n\lambda\theta + n^2\lambda^2}{(\lambda+\theta)^2}\\[10pt]
	&E\left[ \frac{1}{z^2}\right] = \int_0^{\infty} \frac{1}{z^2} \frac{(\lambda+\theta)^n z^{n-1} \exp(-(\lambda+\theta)z)}{\Gamma(n)} \mathrm{d}z = \frac{(\lambda+\theta)^2}{(n-1)(n-2)}
\end{align*}
Then the result is as follows.
\begin{align*}
	Var(\hat{\lambda}) &= E[\hat{\lambda}^2] - (E[\hat{\lambda}])^2 = E\left[ \frac{1}{(\sum_{i=1}^n y_i)^2} \right] E[(\sum_{i=1}^n \delta_i)^2] - (E[\hat{\lambda}])^2\\
	&= \frac{n\lambda\theta + n^2 \lambda^2}{(n-1)(n-2)} - \frac{n^2\lambda^2}{(n-1)^2}
\end{align*}

\subsection{(h)}
First I have to calculate the asymptotic variance of this estimator, which is the inverse of the expected Fisher information from the available data. By using the loglikelihood, the Fisher information of a data point about $\lambda$ is as follows.
\begin{align*}
	I(\lambda) &= E\left[ -\frac{\mathrm{d}^2}{\mathrm{d}\lambda^2} ({\rm log}\ \theta - \lambda y_i - \theta y_i + \delta_i {\rm log}\ \frac{\lambda}{\theta}) \right]\\
	&= E\left[ \frac{\mathrm{d}}{\mathrm{d}\lambda} (-y_i + \frac{\delta_i}{\lambda}) \right]\\
	&= E[\delta_i \lambda^{-2}] = \frac{1}{\lambda(\lambda+\theta)}
\end{align*}
Then the Fisher information of full data is the summation of a data point ones, and from consistency and asymptotic normality of MLE, the asymptotic distribution of $\hat{\lambda}$ is
\begin{align*}
	\hat{\lambda} \stackrel{d}{\longrightarrow} N \left(\lambda, \frac{\lambda(\lambda+\theta)}{n} \right)
\end{align*}
Then by standardization,
\begin{align*}
	\frac{\hat{\lambda} - \lambda}{\sqrt{\frac{\lambda(\lambda+\theta)}{n}}} \stackrel{d}{\longrightarrow} N(0, 1)
\end{align*}
Then, let $z_{\alpha}$ be the $97.5\%$ quantile, since normal distribution is symmetric, in large sample,
\begin{align*}
	{\rm Pr}\left( -z_{\alpha} \leq \frac{\sqrt{n}(\hat{\lambda} - \lambda)}{\sqrt{\lambda(\lambda+\theta)}} \leq z_{\alpha}\right) = 0.95
\end{align*}
Thus the $95\%$ confidence interval for $\lambda$ is $\left[ \hat{\lambda} - z_{\alpha} \sqrt{\frac{\hat{\lambda} (\hat{\lambda}+\theta)}{n}}, \hat{\lambda} + z_{\alpha} \sqrt{\frac{\hat{\lambda} (\hat{\lambda}+\theta)}{n}}\right]$, because of continuous mapping theorem.

\subsection{(i)}
\subsection{(j)}

\end{document}






































