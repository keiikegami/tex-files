\documentclass{article}
\usepackage[margin = .7in]{geometry}
\usepackage[dvipdfmx]{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{bm}
\lstset{%
  language={python},
  basicstyle={\small},%
  identifierstyle={\small},%
  commentstyle={\small\itshape},%
  keywordstyle={\small\bfseries},%
  ndkeywordstyle={\small},%
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},%
  numbers=left,%
  xrightmargin=0zw,%
  xleftmargin=3zw,%
  numberstyle={\scriptsize},%
  stepnumber=1,
  numbersep=1zw,%
  lineskip=-0.5ex%
}

\begin{document}
\title{Econometrics 2 2017 \\ 
Problem set 1}
\author{Kei Ikegami (150012)}
\maketitle

\section{Problem 1}
\subsection{(a)}
Let $\epsilon = y - \alpha - x^{'}\beta$, where $\alpha = E[y] - E[x^{'}]\beta$ and $\beta = \Sigma^{-1}\delta$, then I show $E[\epsilon] = 0$  and $E[\epsilon x] = 0$. 
\begin{align*}
	E[\epsilon] &= E[y - E[y] + E[x^{'}]\beta - x^{'}\beta] = E[(y - E[y]) - (x^{'} - E[x^{'}])\beta] = (E[y] - E[y]) - (E[x^{'}] - E[x^{'}])\beta = 0\\[8pt]
	E[\epsilon x] &= E[x\epsilon] = E[x(y - E[y] + E[x^{'}]\beta - x^{'}\beta)] = E[x(y - E[y]) - x(x^{'} - E[x^{'}])\beta]\\[8pt]
	&= E[(x - E[x])(y - E[y]) + E[x](y - E[y]) - (x - E[x])(x^{'} - E[x^{'}])\beta - E[x](x^{'} - E[x^{'}])\beta]\\[8pt]
	&= \delta - E[(x - E[x])(x^{'} - E[x^{'}])]\Sigma^{-1}\delta = \delta - \delta = 0
\end{align*}
So now I get the result.

\subsection{(b)}
This transformation is useful.
\begin{align*}
	E[(y - a - x^{'}b)^2] = E[(y - E[y])^2] + 2E[(y- E[y])(E[y] - (a + x^{'}b))] + E[(E[y] - (a + x^{'}b))^2]
\end{align*}
Then I get the FOC by differentiating by $a$ as follows.
\begin{align*}
	E[-2(y - E[y])] + E[-2(E[y] - (a + x^{'}b))] = 0\ \Rightarrow \ a = E[y] -E[x^{'}]b
\end{align*}
Next, after inserting the above relationship to the original, I get the FOC by differentiating by $b$ as follows.
\begin{align*}
	-2E[(y - E[y])(x^{'} - E[x^{'}])]^{'} + 2E[(x^{'} - E[x^{'}])^{'}(x^{'} - E[x^{'}])b] = 0\ \Leftrightarrow \ b = \Sigma^{-1}\delta
\end{align*}
And the second derivative by $b$ is $2 \Sigma$, which is positive semi definite, then the second order condition for minimization is fulfilled. Thus I show $\alpha, \beta$ in (a) solves this minimization problem.

Then I show the second part. First I show the important property of the conditional expectation. If $y = E[y|x] + \epsilon$, then $E[\epsilon | x] = E[y - E[y|x] |x] = 0$ and for any function $h(x)$, $E[h(x) \epsilon] = E[E[h(x)\epsilon | x]] = E[h(x)E[\epsilon | x]] = 0$. Using this second property can easily prove the argument.
\begin{align*}
	E[(y - a - x^{'}b)^2] &= E[(y- E[y|x] + E[y|x] -a -x^{'}b)^2] \\[8pt]
	&= E[(y- E[y|x])^2] + 2E[(y - E[y])(E[y|x] -a-x^{'}b)] + E[(E[y|x] - a-x^{'}b)^2]\\[8pt]
	&= E[(y- E[y|x])^2] + E[(E[y|x] - a-x^{'}b)^2]
\end{align*}
The second term in the first line vanishes since $E[(y - E[y])(E[y|x] -a-x^{'}b)] = E[\epsilon(E[y|x] -a-x^{'}b)] = 0$. This is because $E[y|x] -a-x^{'}b$ is just a function of $x$.

\subsection{(c)}
In the real econometric analysis, the relationship $E[\epsilon x] = 0$ is just an assumption. And that the correlation between the regressor and the error is zero needs the situation where all relevant variables are in the regression model, but this is unrealistic because certainly many unobservable variables exist. We can use IV for solving such a situation and get the consistent estimator of the coefficient of interesting variables.

\subsection{(d)}
(b) says that the predicted value by using OLS estimator is the best linear approximate of the conditional mean of the dependent variable even if the form is not linear among the support, when we use squared loss function. This clearly argue the statement.

\subsection{(e)}
houti

\section{Problem 2}
\subsection{(a)}
\begin{align*}
	Cov(Az_i, \epsilon_i) = E[(Az_i - E[Az_i])(\epsilon_i - E[\epsilon_i])] = A E[(z_i - E[z_i])(\epsilon_i - E[\epsilon_i])] = 0
\end{align*}
Therefore $Az_i$ does not correlate with $\epsilon_i$. And clearly the correlation exist between $Az_i$ and $x_i$. Thus $Az_i$ is a valid IV for $x_i$. And $rank(E[Az_i x_i^{'}]) = K$ allows it to have inverse matrix, so IV estimator can be constructed.

\subsection{(b)}
Multiplying $Az_i$ to the first model by left.
\begin{align*}
	Az_iy_i = Az_ix_i^{'}\beta + Az_i\epsilon_i
\end{align*}
Sum up the model by individuals. And divide by $n$.
\begin{align*}
	\sum_{i = 1}^n Az_iy_i = (\sum_{i = 1}^n Az_ix_i^{'})\beta + \sum_{i=1}^nAz_i\epsilon_i \ \Leftrightarrow\ \left(\frac{1}{n}\sum_{i = 1}^n Az_iy_i\right) = \left(\frac{1}{n}\sum_{i = 1}^n Az_ix_i^{'}\right)\beta + \left( \frac{1}{n}\sum_{i=1}^nAz_i\epsilon_i \right)
\end{align*}
Consider the moment condition $E[Az_i\epsilon_i] = 0$. And we have the assumption that $rank(E[Az_i x_i^{'}]) = K$, so the sample analogue of this moment can be inversed. Then I have the IV estimator as the method of moment estimator as follows.
\begin{align*}
	\hat{\beta_A} = \left(\frac{1}{n}\sum_{i = 1}^n Az_ix_i^{'}\right)^{-1} \left(\frac{1}{n}\sum_{i = 1}^n Az_iy_i\right)
\end{align*}

\subsection{(c)}
Using the matrix $A$, I can write the sample moment condition as follows.
\begin{align*}
	\frac{1}{n}\sum_{i = 1}^n \left(\sum_{i = 1}^n x_i z_i^{'}\right)\left(\sum_{i = 1}^n z_i z_i^{'}\right)^{-1} z_i (y_i - x_i^{'}b) = 0
\end{align*}
And I use the following matrix notation to make it easy to see, $\sum_{i = 1}^n x_i z_i^{'} = X^{'}Z$ and $\sum_{i = 1}^n z_i z_i^{'} = Z^{'}Z$. Then the condition leads to the IV estimator.
\begin{align*}
	&\frac{1}{n}\sum_{i = 1}^n \left(X^{'}Z\right)\left(Z^{'}Z\right)^{-1} z_i (y_i - x_i^{'}b) = 0\\[8pt]
	\quad \Leftrightarrow \quad&\sum_{i=1}^n \left(X^{'}Z\right)\left(Z^{'}Z\right)^{-1} z_i y_i = \sum_{i=1}^n \left(X^{'}Z\right)\left(Z^{'}Z\right)^{-1}z_i x_i^{'}b\\[8pt]
	\quad \Leftrightarrow \quad& \left(X^{'}Z\right)\left(Z^{'}Z\right)^{-1}\left(Z^{'}y\right) = \left(X^{'}Z\right)\left(Z^{'}Z\right)^{-1}\left(Z^{'}X\right)b\\[8pt]
	\quad \Leftrightarrow \quad&b = (X^{'}P_ZX)^{-1}X^{'}P_Z y
\end{align*}

\subsection{(d)}
Using matrix notations lead to the result. Note that $Z^{'}X$ has its inverse matrix because $L = K$.
\begin{align*}
	\hat{\beta_A} = (AZ^{'}X)^{-1}AZ^{'}y = (Z^{'}X)^{-1}A^{-1}AZ^{'}y = (Z^{'}X)^{-1}Z^{'}y
\end{align*}
The last term shows it does not depend on $A$.

\section{Problem 3}
\subsection{(a)}
\subsection{(b)}
\subsection{(c)}
\subsection{(d)}
\subsection{(e)}

\section{Problem 4}
\subsection{(a)}
The model tells us that
\begin{align*}
	Y_1 = X\beta_1 + E_1\\
	Y_2 = X\beta_2 + E_2
\end{align*}
Therefore it is clear that all in one matrix formulation is the mentioned form.

\subsection{(b)}
$\sigma_{ab}$ denotes the covariance between $\epsilon_{i,a}$ and $\epsilon_{i, b}$. Note that the errors of different individuals do not correlate. The matrix $\Sigma$ is
\begin{align*}
\Sigma = \begin{pmatrix}
\sigma_{11} & \sigma_{12}\\
\sigma_{21} & \sigma_{22}
\end{pmatrix}
\end{align*}

 Let $E$ be the joint errors. Therefore the variance covariance matrix in this model is as follows.
\begin{align*}
	Var\left(E \right) = \begin{pmatrix}\sigma_{11}& 0& \cdots& 0&\sigma_{12}&0 &\cdots & 0\\
	\vdots&&&\vdots&\vdots&&&\vdots\\
	0&0&\cdots&\sigma_{11}&0&0&\cdots&\sigma_{12}\\
	\sigma_{21}& 0& \cdots& 0&\sigma_{22}&0 &\cdots & 0\\
	\vdots&&&\vdots&\vdots&&&\vdots\\
	0&0&\cdots&\sigma_{21}&0&0&\cdots&\sigma_{22}
	\end{pmatrix}
	=\Sigma \otimes I_N
\end{align*}

\subsection{(c)}
By the definition of GLS estimator, I get in this case as follows.
\begin{align*}
	\beta_{GLS} = \left(\left(\begin{array}{cc} X & 0 \\ 0 & X \end{array}\right)^{'} \left(\Sigma \otimes I_N \right)^{-1} \left(\begin{array}{cc} X & 0 \\ 0 & X \end{array}\right)\right)^{-1} \left(\begin{array}{cc} X & 0 \\ 0 & X \end{array}\right)^{'} \left(\Sigma \otimes I_N \right)^{-1} Y
\end{align*}
when $Y = \left(\begin{array}{cc} Y_1\\Y_2 \end{array}\right)$

\subsection{(d)}
By (c)
\begin{align*}
	\beta_{GLS} &= \left(\left(\begin{array}{cc} X^{'} & 0 \\ 0 & X^{'} \end{array}\right) \left(\Sigma^{-1} \otimes I_N \right) \left(\begin{array}{cc} X & 0 \\ 0 & X \end{array}\right)\right)^{-1} \left(\begin{array}{cc} X^{'} & 0 \\ 0 & X^{'} \end{array}\right) \left(\Sigma^{-1} \otimes I_N \right) Y\\[8pt]
	&= \left(\left(\begin{array}{cc} X^{'} & 0 \\ 0 & X^{'} \end{array}\right) \left(\begin{array}{cc} \sigma_{22}I_N & -\sigma_{12}I_N \\ -\sigma_{21} I_N & \sigma_{11} I_N \end{array}\right) \left(\begin{array}{cc} X & 0 \\ 0 & X \end{array}\right)\right)^{-1} \left(\begin{array}{cc} X^{'} & 0 \\ 0 & X^{'} \end{array}\right) \left(\begin{array}{cc} \sigma_{22}I_N & -\sigma_{12}I_N \\ -\sigma_{21} I_N & \sigma_{11} I_N \end{array}\right) Y\\[8pt]
	&= \left( \begin{array}{cc} \sigma_{22}X^{'}X & -\sigma_{12} X^{'}X \\ -\sigma_{21}X^{'}X & \sigma_{11}X^{'}X \end{array} \right)^{-1} \left( \begin{array}{cc} \sigma_{22}X^{'} & -\sigma_{12} X^{'} \\ -\sigma_{21}X^{'} & \sigma_{11}X^{'} \end{array} \right) y\\[8pt]
	&= \left(\left( \begin{array}{cc} \sigma_{22} & -\sigma_{12} \\ -\sigma_{21} & \sigma_{11} \end{array} \right) \otimes X^{'}X\right)^{-1} \left(\left( \begin{array}{cc} \sigma_{22} & -\sigma_{12} \\ -\sigma_{21} & \sigma_{11} \end{array} \right) \otimes X^{'} \right) y\\[8pt]
	&= \left( I_2 \otimes (X^{'}X)^{-1}X^{'} \right) y\\[8pt]
	&= \left( \begin{array}{cc} (X^{'}X)^{-1}X^{'} & 0 \\ 0 & (X^{'}X)^{-1}X^{'} \end{array} \right) \left( \begin{array}{cc} Y_1\\Y_2 \end{array}\right)\\[8pt]
	&= \left( \begin{array}{cc} (X^{'}X)^{-1}X^{'}Y_1 \\ (X^{'}X)^{-1}X^{'}Y_2 \end{array}\right)
\end{align*}
The second equality is from $\Sigma^{-1} = \frac{1}{\sigma_{11}\sigma_{22} - \sigma_{12}^2}\left( \begin{array}{cc} \sigma_{22} & -\sigma_{12} \\ -\sigma_{21} & \sigma_{11} \end{array} \right)$. By the above I get the result.


\section{Problem 5}
\subsection{(a)}
\subsection{(b)}

\end{document}
























