\documentclass{article}
\usepackage[margin = .7in]{geometry}
\usepackage[dvipdfmx]{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{bm}
\lstset{%
  language={python},
  basicstyle={\small},%
  identifierstyle={\small},%
  commentstyle={\small\itshape},%
  keywordstyle={\small\bfseries},%
  ndkeywordstyle={\small},%
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},%
  numbers=left,%
  xrightmargin=0zw,%
  xleftmargin=3zw,%
  numberstyle={\scriptsize},%
  stepnumber=1,
  numbersep=1zw,%
  lineskip=-0.5ex%
}

\begin{document}
\title{Econometrics 2 2017 \\ 
Problem set 1}
\author{Kei Ikegami (150012)}
\maketitle

\section{Problem 1}
\subsection{(a)}
Let $\epsilon = y - \alpha - x^{'}\beta$, where $\alpha = E[y] - E[x^{'}]\beta$ and $\beta = \Sigma^{-1}\delta$, then I show $E[\epsilon] = 0$  and $E[\epsilon x] = 0$. 
\begin{align*}
	E[\epsilon] &= E[y - E[y] + E[x^{'}]\beta - x^{'}\beta] = E[(y - E[y]) - (x^{'} - E[x^{'}])\beta] = (E[y] - E[y]) - (E[x^{'}] - E[x^{'}])\beta = 0\\[8pt]
	E[\epsilon x] &= E[x\epsilon] = E[x(y - E[y] + E[x^{'}]\beta - x^{'}\beta)] = E[x(y - E[y]) - x(x^{'} - E[x^{'}])\beta]\\[8pt]
	&= E[(x - E[x])(y - E[y]) + E[x](y - E[y]) - (x - E[x])(x^{'} - E[x^{'}])\beta - E[x](x^{'} - E[x^{'}])\beta]\\[8pt]
	&= \delta - E[(x - E[x])(x^{'} - E[x^{'}])]\Sigma^{-1}\delta = \delta - \delta = 0
\end{align*}
So now I get the result.

\subsection{(b)}
This transformation is useful.
\begin{align*}
	E[(y - a - x^{'}b)^2] = E[(y - E[y])^2] + 2E[(y- E[y])(E[y] - (a + x^{'}b))] + E[(E[y] - (a + x^{'}b))^2]
\end{align*}
Then I get the FOC by differentiating by $a$ as follows.
\begin{align*}
	E[-2(y - E[y])] + E[-2(E[y] - (a + x^{'}b))] = 0\ \Rightarrow \ a = E[y] -E[x^{'}]b
\end{align*}
Next, after inserting the above relationship to the original, I get the FOC by differentiating by $b$ as follows.
\begin{align*}
	-2E[(y - E[y])(x^{'} - E[x^{'}])]^{'} + 2E[(x^{'} - E[x^{'}])^{'}(x^{'} - E[x^{'}])b] = 0\ \Leftrightarrow \ b = \Sigma^{-1}\delta
\end{align*}
And the second derivative by $b$ is $2 \Sigma$, which is positive semi definite, then the second order condition for minimization is fulfilled. Thus I show $\alpha, \beta$ in (a) solves this minimization problem.

Then I show the second part. First I show the important property of the conditional expectation. If $y = E[y|x] + \epsilon$, then $E[\epsilon | x] = E[y - E[y|x] |x] = 0$ and for any function $h(x)$, $E[h(x) \epsilon] = E[E[h(x)\epsilon | x]] = E[h(x)E[\epsilon | x]] = 0$. Using this second property can easily prove the argument.
\begin{align*}
	
\end{align*}

\subsection{(c)}
\subsection{(d)}
\subsection{(e)}

\section{Problem 2}
\subsection{(a)}
\subsection{(b)}
\subsection{(c)}
\subsection{(d)}

\section{Problem 3}
\subsection{(a)}
\subsection{(b)}
\subsection{(c)}
\subsection{(d)}
\subsection{(e)}

\section{Problem 4}
\subsection{(a)}
\subsection{(b)}
\subsection{(c)}
\subsection{(d)}

\section{Problem 5}
\subsection{(a)}
\subsection{(b)}

\end{document}

















