\documentclass{article}
\usepackage[margin = .7in]{geometry}
\usepackage[dvipdfmx]{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{bm}
\lstset{%
  language={python},
  basicstyle={\small},%
  identifierstyle={\small},%
  commentstyle={\small\itshape},%
  keywordstyle={\small\bfseries},%
  ndkeywordstyle={\small},%
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},%
  numbers=left,%
  xrightmargin=0zw,%
  xleftmargin=3zw,%
  numberstyle={\scriptsize},%
  stepnumber=1,
  numbersep=1zw,%
  lineskip=-0.5ex%
}

\begin{document}
\title{Econometrics 2 2017 \\ 
Problem set 1}
\author{Kei Ikegami (150012)}
\maketitle

\section{Problem 1}
\subsection{(a)}
Let $\epsilon = y - \alpha - x^{'}\beta$, where $\alpha = E[y] - E[x^{'}]\beta$ and $\beta = \Sigma^{-1}\delta$, then I show $E[\epsilon] = 0$  and $E[\epsilon x] = 0$. 
\begin{align*}
	E[\epsilon] &= E[y - E[y] + E[x^{'}]\beta - x^{'}\beta] = E[(y - E[y]) - (x^{'} - E[x^{'}])\beta] = (E[y] - E[y]) - (E[x^{'}] - E[x^{'}])\beta = 0\\[8pt]
	E[\epsilon x] &= E[x\epsilon] = E[x(y - E[y] + E[x^{'}]\beta - x^{'}\beta)] = E[x(y - E[y]) - x(x^{'} - E[x^{'}])\beta]\\[8pt]
	&= E[(x - E[x])(y - E[y]) + E[x](y - E[y]) - (x - E[x])(x^{'} - E[x^{'}])\beta - E[x](x^{'} - E[x^{'}])\beta]\\[8pt]
	&= \delta - E[(x - E[x])(x^{'} - E[x^{'}])]\Sigma^{-1}\delta = \delta - \delta = 0
\end{align*}
So now I get the result.

\subsection{(b)}
This transformation is useful.
\begin{align*}
	E[(y - a - x^{'}b)^2] = E[(y - E[y])^2] + 2E[(y- E[y])(E[y] - (a + x^{'}b))] + E[(E[y] - (a + x^{'}b))^2]
\end{align*}
Then I get the FOC by differentiating by $a$ as follows.
\begin{align*}
	E[-2(y - E[y])] + E[-2(E[y] - (a + x^{'}b))] = 0\ \Rightarrow \ a = E[y] -E[x^{'}]b
\end{align*}
Next, after inserting the above relationship to the original, I get the FOC by differentiating by $b$ as follows.
\begin{align*}
	-2E[(y - E[y])(x^{'} - E[x^{'}])]^{'} + 2E[(x^{'} - E[x^{'}])^{'}(x^{'} - E[x^{'}])b] = 0\ \Leftrightarrow \ b = \Sigma^{-1}\delta
\end{align*}
And the second derivative by $b$ is $2 \Sigma$, which is positive semi definite, then the second order condition for minimization is fulfilled. Thus I show $\alpha, \beta$ in (a) solves this minimization problem.

Then I show the second part. First I show the important property of the conditional expectation. If $y = E[y|x] + \epsilon$, then $E[\epsilon | x] = E[y - E[y|x] |x] = 0$ and for any function $h(x)$, $E[h(x) \epsilon] = E[E[h(x)\epsilon | x]] = E[h(x)E[\epsilon | x]] = 0$. Using this second property can easily prove the argument.
\begin{align*}
	E[(y - a - x^{'}b)^2] &= E[(y- E[y|x] + E[y|x] -a -x^{'}b)^2] \\[8pt]
	&= E[(y- E[y|x])^2] + 2E[(y - E[y])(E[y|x] -a-x^{'}b)] + E[(E[y|x] - a-x^{'}b)^2]\\[8pt]
	&= E[(y- E[y|x])^2] + E[(E[y|x] - a-x^{'}b)^2]
\end{align*}
The second term in the first line vanishes since $E[(y - E[y])(E[y|x] -a-x^{'}b)] = E[\epsilon(E[y|x] -a-x^{'}b)] = 0$. This is because $E[y|x] -a-x^{'}b$ is just a function of $x$.

\subsection{(c)}
In the real econometric analysis, the relationship $E[\epsilon x] = 0$ is just an assumption. And that the correlation between the regressor and the error is zero needs the situation where all relevant variables are in the regression model, but this is unrealistic because certainly many unobservable variables exist. We can use IV for solving such a situation and get the consistent estimator of the coefficient of interesting variables.

\subsection{(d)}
(b) says that the population regression function is the best linear approximate of the conditional mean of the dependent variable even if the form is not linear among the support, when we use squared loss function. So we have to show the OLS estimator consistently estimates the coefficients.  Let $\beta_{OLS}$ be the OLS estimator of the constant and coefficients.
\begin{align*}
	\beta_{OLS} = \left( {\bf X}^{'}{\bf X} \right)^{-1}{\rm X}^{'} {\rm y}
\end{align*}
where ${\bf X} = (x_1, \cdots, x_n)^{'},\ {\rm y} = (y_1, \cdots, y_n)^{'}$. So this is the sample analogue of $E\left[ X_i X_i^{'} \right]^{-1}E\left[ X_i y_i \right]$. By LLN, we can estimate consistently this coefficients. So the remain is to prove the equality between $E\left[ X_i X_i^{'} \right]^{-1}E\left[ X_i y_i \right]$ and $\left(\begin{array}{cc} E[y] - E[x^{'}]\beta\\ \Sigma^{-1}\delta \end{array}\right)$. Note that $x_i$ contains $1$ whose coefficient is the constant term, while $x$ contains only variables.

We show $E\left[ X_i y_i \right] = E\left[ X_i X_i^{'} \right] \left(\begin{array}{cc} E[y] - E[x^{'}]\beta\\ \Sigma^{-1}\delta \end{array}\right)$.

\begin{align*}
	E\left[ X_i X_i^{'} \right] \left(\begin{array}{cc} E[y] - E[x^{'}]\beta\\ \Sigma^{-1}\delta \end{array}\right) &= \left(\begin{array}{cc} 1 & E[x^{'}] \\ E[x] & \Sigma + E[x]E[x^{'}] \end{array}\right)  \left(\begin{array}{cc} E[y] - E[x^{'}]\beta \\ \Sigma^{-1}\delta \end{array}\right)\\[8pt]
	&=  \left(\begin{array}{cc} E[y - E[x^{'}]]\beta + E[x^{'}]\beta \\[6pt] E[x](E[y] - E[x^{'}]\beta) + (\Sigma + E[x]E[x^{'}])\beta \end{array}\right)\\[8pt]
	&=  \left(\begin{array}{cc} E[y] \\ E[y]E[x] + \delta \end{array}\right)\\[8pt]
	&=  \left(\begin{array}{cc} E[y_i] \\ E[x_{i1}, y_i] \\ \vdots \\ E[x_{ik}y_i] \end{array}\right)
\end{align*}
Now we have the ideal result.

\subsection{(e)}
\begin{description}
	\item[(1)] We have no evidence for the accuracy of approximation outside the support.
	\item[(2)] I assume there are infinite data in both of cases. Taylor expansion of $E[y | x_1, x_2]$ around $(x_1 = x, x_2 = x)$ is as follows.
	\begin{align*}
		E[y | x_1^*, x_2^*] \approx E[y | x_1 = x, x_2 = x] + (x_1^* - x) \left. \frac{\partial}{\partial x_1} E[y | x_1, x_2] \right|_{x_1 = x_2 = x} + (x_2^* - x)\left. \frac{\partial}{\partial x_2} E[y | x_1, x_2] \right|_{x_1 = x_2 = x} 
	\end{align*}
	I omit the error term. Now we know the OLS estimator provides the best linear approximation of the conditional expectation, i.e. $E[y | x_1, x_2]$. So the first derivative terms in the above approximation can be seen replaced with each coefficients resulted from OLS in the prediction. And we need the derivative value just around $x_1 = x_2 = x$ if we want to see the behavior of $E[y | x_1, x_2]$ near $x_1 = x_2$. When we use the data only around $x_1 = x_2$, the derived coefficients are fitted for the data restricted around $x_1 = x_2$. On the other hand, when we use the data distributed overall, the ones have to be fitted for the data far away from $x_1 = x_2$. This means the approximation to $\left. \frac{\partial}{\partial x_1} E[y | x_1, x_2] \right|_{x_1 = x_2 = x}$ and $\left. \frac{\partial}{\partial x_2} E[y | x_1, x_2] \right|_{x_1 = x_2 = x}$ by OLS estimators is better in the second case than in the first case. If we want to know the data behavior away from $x_1 = x_2$, this is not true, because the data support of the second case is just around $x_1 = x_2$ and then the case have no knowledge about the data away from the 45 line, which are outside the support. Thus in this case the first data set provides the better approximation.
\end{description}

\section{Problem 2}
\subsection{(a)}
\begin{align*}
	Cov(Az_i, \epsilon_i) = E[(Az_i - E[Az_i])(\epsilon_i - E[\epsilon_i])] = A E[(z_i - E[z_i])(\epsilon_i - E[\epsilon_i])] = 0
\end{align*}
Therefore $Az_i$ does not correlate with $\epsilon_i$. And clearly the correlation exist between $Az_i$ and $x_i$. Thus $Az_i$ is a valid IV for $x_i$. And $rank(E[Az_i x_i^{'}]) = K$ allows it to have inverse matrix, so IV estimator can be constructed.

\subsection{(b)}
Multiplying $Az_i$ to the first model by left.
\begin{align*}
	Az_iy_i = Az_ix_i^{'}\beta + Az_i\epsilon_i
\end{align*}
Sum up the model by individuals. And divide by $n$.
\begin{align*}
	\sum_{i = 1}^n Az_iy_i = (\sum_{i = 1}^n Az_ix_i^{'})\beta + \sum_{i=1}^nAz_i\epsilon_i \ \Leftrightarrow\ \left(\frac{1}{n}\sum_{i = 1}^n Az_iy_i\right) = \left(\frac{1}{n}\sum_{i = 1}^n Az_ix_i^{'}\right)\beta + \left( \frac{1}{n}\sum_{i=1}^nAz_i\epsilon_i \right)
\end{align*}
Consider the moment condition $E[Az_i\epsilon_i] = 0$. And we have the assumption that $rank(E[Az_i x_i^{'}]) = K$, so the sample analogue of this moment can be inversed. Then I have the IV estimator as the method of moment estimator as follows.
\begin{align*}
	\hat{\beta_A} = \left(\frac{1}{n}\sum_{i = 1}^n Az_ix_i^{'}\right)^{-1} \left(\frac{1}{n}\sum_{i = 1}^n Az_iy_i\right)
\end{align*}

\subsection{(c)}
Using the matrix $A$, I can write the sample moment condition as follows.
\begin{align*}
	\frac{1}{n}\sum_{i = 1}^n \left(\sum_{i = 1}^n x_i z_i^{'}\right)\left(\sum_{i = 1}^n z_i z_i^{'}\right)^{-1} z_i (y_i - x_i^{'}b) = 0
\end{align*}
And I use the following matrix notation to make it easy to see, $\sum_{i = 1}^n x_i z_i^{'} = X^{'}Z$ and $\sum_{i = 1}^n z_i z_i^{'} = Z^{'}Z$. Then the condition leads to the IV estimator.
\begin{align*}
	&\frac{1}{n}\sum_{i = 1}^n \left(X^{'}Z\right)\left(Z^{'}Z\right)^{-1} z_i (y_i - x_i^{'}b) = 0\\[8pt]
	\quad \Leftrightarrow \quad&\sum_{i=1}^n \left(X^{'}Z\right)\left(Z^{'}Z\right)^{-1} z_i y_i = \sum_{i=1}^n \left(X^{'}Z\right)\left(Z^{'}Z\right)^{-1}z_i x_i^{'}b\\[8pt]
	\quad \Leftrightarrow \quad& \left(X^{'}Z\right)\left(Z^{'}Z\right)^{-1}\left(Z^{'}y\right) = \left(X^{'}Z\right)\left(Z^{'}Z\right)^{-1}\left(Z^{'}X\right)b\\[8pt]
	\quad \Leftrightarrow \quad&b = (X^{'}P_ZX)^{-1}X^{'}P_Z y
\end{align*}

\subsection{(d)}
Using matrix notations lead to the result. Note that $Z^{'}X$ has its inverse matrix because $L = K$.
\begin{align*}
	\hat{\beta_A} = (AZ^{'}X)^{-1}AZ^{'}y = (Z^{'}X)^{-1}A^{-1}AZ^{'}y = (Z^{'}X)^{-1}Z^{'}y
\end{align*}
The last term shows it does not depend on $A$.

\section{Problem 3}
I show the result of the regression.
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{3.png}
    \caption{Results of the three regressions}
\end{figure}
Column 1 is the result of the first type of regression, which I use just a one year data and OLS. Column 2 is the second one, which I use all the data and fixed effect estimator. Column 3 is the third one, which I use different data in that $x_{2it} = \alpha_i + 0.1u_{2it}$.
\subsection{(a)}
I use Python in this problem for convenience.

\subsection{(b)}
Constant term and the coefficient of $z_i$ is consistently estimated in this case and the coefficients of $x_{1it}, x_{2it}$ is biased. This is because the first two components do not correlate with the error term while the last two correlates through $\alpha_i$. I see the coefficients of the last two is positively biased, in other words they are bigger than true value $1$. And this is caused by the positive correlations with error term.

\subsection{(c)}
This is done in the first table.

\subsection{(d)}
In this regression, while the estimate of the coefficient of $x_{1it}$ seems to be correct, the coefficient of $x_{2it}$ is positively biased and furthermore has a large standard error as in the first table. I explain this.

In the original data, $y_{it} = 1 + x_{1it} + x_{2it} + z_i + \alpha_i + \nu_{it} = y_{it} = 1 + \left( \alpha_i + u_{1it} \right) + \left( \alpha_i + u_{2it} \right) + z_i + \alpha_i + \nu_{it}$. And in this problem, $y_{it} = 1 + \left( \alpha_i + u_{1it} \right) + \left( \alpha_i + 0.1 u_{2it} \right) + z_i + \alpha_i + \nu_{it}$. Remember the fixed effect estimator is the same as the within estimator, which is given by the regression using each observation's gap from each individual mean . Then I have the variance of the estimator as in the below form, when $\beta_k^l$ denotes the estimated coefficient of $x_{kit}$ in data $l\ \in \left\{ c, d\right\}$.
\begin{align*}
	&Var\left( \beta_2^c \right) = \frac{\sigma^2}{\sum_i (u_{2it} - \bar{u_{2i}})^2}\\
	&Var\left( \beta_2^d \right) = \frac{100\sigma^2}{\sum_i (u_{2it} - \bar{u_{2i}})^2}
\end{align*}
where $\sigma^2 = Var(\nu_{it} - \bar{\nu_i})$. $100$ in the second one is from $0.1 u_{2it}$. This reveals the standard deviation in the data $d$ is ten times larger than one in the data $c$. And this is true as you see in the first table.

\subsection{(e)}
There is an estimated constant in STATA result as in the first table, but this is just for convenience and calculated as the mean of individual effect. Constant term and the coefficients of variables which take the same value for each individual cannot be estimated by FE estimator, because the effect of such variables to the dependent variable cannot be separated by the individual effects. This is clear as you see in the below within regression, which produces the same result of FE estimator.
\begin{align*}
	(y_{it} - \bar{y_i}) = \beta_1 (u_{1it} - \bar{u_{1i}}) + \beta_2 (u_{2it} - \bar{u_2i}) + (\nu_{it} - \bar{\nu_i})
\end{align*}

Constant term and $z_i$ is not in the above regression, so it is natural that you cannot obtain the estimated coefficients of them.


\section{Problem 4}
\subsection{(a)}
The model tells us that
\begin{align*}
	Y_1 = X\beta_1 + E_1\\
	Y_2 = X\beta_2 + E_2
\end{align*}
Therefore it is clear that all in one matrix formulation is the mentioned form.

\subsection{(b)}
$\sigma_{ab}$ denotes the covariance between $\epsilon_{i,a}$ and $\epsilon_{i, b}$. Note that the errors of different individuals do not correlate. The matrix $\Sigma$ is
\begin{align*}
\Sigma = \begin{pmatrix}
\sigma_{11} & \sigma_{12}\\
\sigma_{21} & \sigma_{22}
\end{pmatrix}
\end{align*}

 Let $E$ be the joint errors. Therefore the variance covariance matrix in this model is as follows.
\begin{align*}
	Var\left(E \right) = \begin{pmatrix}\sigma_{11}& 0& \cdots& 0&\sigma_{12}&0 &\cdots & 0\\
	\vdots&&&\vdots&\vdots&&&\vdots\\
	0&0&\cdots&\sigma_{11}&0&0&\cdots&\sigma_{12}\\
	\sigma_{21}& 0& \cdots& 0&\sigma_{22}&0 &\cdots & 0\\
	\vdots&&&\vdots&\vdots&&&\vdots\\
	0&0&\cdots&\sigma_{21}&0&0&\cdots&\sigma_{22}
	\end{pmatrix}
	=\Sigma \otimes I_N
\end{align*}

\subsection{(c)}
By the definition of GLS estimator, I get in this case as follows.
\begin{align*}
	\beta_{GLS} = \left(\left(\begin{array}{cc} X & 0 \\ 0 & X \end{array}\right)^{'} \left(\Sigma \otimes I_N \right)^{-1} \left(\begin{array}{cc} X & 0 \\ 0 & X \end{array}\right)\right)^{-1} \left(\begin{array}{cc} X & 0 \\ 0 & X \end{array}\right)^{'} \left(\Sigma \otimes I_N \right)^{-1} Y
\end{align*}
when $Y = \left(\begin{array}{cc} Y_1\\Y_2 \end{array}\right)$

\subsection{(d)}
By (c)
\begin{align*}
	\beta_{GLS} &= \left(\left(\begin{array}{cc} X^{'} & 0 \\ 0 & X^{'} \end{array}\right) \left(\Sigma^{-1} \otimes I_N \right) \left(\begin{array}{cc} X & 0 \\ 0 & X \end{array}\right)\right)^{-1} \left(\begin{array}{cc} X^{'} & 0 \\ 0 & X^{'} \end{array}\right) \left(\Sigma^{-1} \otimes I_N \right) Y\\[8pt]
	&= \left(\left(\begin{array}{cc} X^{'} & 0 \\ 0 & X^{'} \end{array}\right) \left(\begin{array}{cc} \sigma_{22}I_N & -\sigma_{12}I_N \\ -\sigma_{21} I_N & \sigma_{11} I_N \end{array}\right) \left(\begin{array}{cc} X & 0 \\ 0 & X \end{array}\right)\right)^{-1} \left(\begin{array}{cc} X^{'} & 0 \\ 0 & X^{'} \end{array}\right) \left(\begin{array}{cc} \sigma_{22}I_N & -\sigma_{12}I_N \\ -\sigma_{21} I_N & \sigma_{11} I_N \end{array}\right) Y\\[8pt]
	&= \left( \begin{array}{cc} \sigma_{22}X^{'}X & -\sigma_{12} X^{'}X \\ -\sigma_{21}X^{'}X & \sigma_{11}X^{'}X \end{array} \right)^{-1} \left( \begin{array}{cc} \sigma_{22}X^{'} & -\sigma_{12} X^{'} \\ -\sigma_{21}X^{'} & \sigma_{11}X^{'} \end{array} \right) y\\[8pt]
	&= \left(\left( \begin{array}{cc} \sigma_{22} & -\sigma_{12} \\ -\sigma_{21} & \sigma_{11} \end{array} \right) \otimes X^{'}X\right)^{-1} \left(\left( \begin{array}{cc} \sigma_{22} & -\sigma_{12} \\ -\sigma_{21} & \sigma_{11} \end{array} \right) \otimes X^{'} \right) y\\[8pt]
	&= \left( I_2 \otimes (X^{'}X)^{-1}X^{'} \right) y\\[8pt]
	&= \left( \begin{array}{cc} (X^{'}X)^{-1}X^{'} & 0 \\ 0 & (X^{'}X)^{-1}X^{'} \end{array} \right) \left( \begin{array}{cc} Y_1\\Y_2 \end{array}\right)\\[8pt]
	&= \left( \begin{array}{cc} (X^{'}X)^{-1}X^{'}Y_1 \\ (X^{'}X)^{-1}X^{'}Y_2 \end{array}\right)
\end{align*}
The second equality is from $\Sigma^{-1} = \frac{1}{\sigma_{11}\sigma_{22} - \sigma_{12}^2}\left( \begin{array}{cc} \sigma_{22} & -\sigma_{12} \\ -\sigma_{21} & \sigma_{11} \end{array} \right)$. By the above I get the result.


\section{Problem 5}
\subsection{(a)}
In this case, $\Sigma = \left( \begin{array}{ccc} \sigma_{11}&\cdots&\sigma_{1T}\\ \vdots&&\vdots\\ \sigma_{1T}&\cdots&\sigma_{TT} \end{array} \right)$, where $\sigma_{tt^{'}} = E[\epsilon_{it} \epsilon_{it^{'}} | x_{it}, z_{it^{'}}]$. Using this notations,
\begin{align*}
	Var(E | X) = I_N \otimes \Sigma
\end{align*}

\subsection{(b)}
\begin{align*}
	M &= I_{NT} - \left( I_N \otimes \iota_T \right) \left( \left( I_N \otimes \iota_T \right)^{'} \left( I_N \otimes \iota_T \right) \right)^{-1} \left( I_N \otimes \iota_T \right)^{'}\\[8pt]
	&= I_{NT} - \left( I_N \otimes \iota_T \right)(T\cdot I_N)^{-1} \left( I_N \otimes \iota_T \right)^{'}\\[8pt]
	&= I_{NY} - \frac{1}{T}\left( I_N \otimes \iota_T\iota_T^{'}\right)
\end{align*}
Then,
\begin{align*}
	My &= y - \frac{1}{T}\left( I_N \otimes \iota_T\iota_T^{'}\right)y\\[8pt]
\end{align*}
Now I focus on the first $T$×$T$ elements in $I_N \otimes \iota_T\iota_T^{'}$, which is $\iota_T\iota_T^{'}$. When the first $T$ elements in
$y$ is denoted as $y_1$, the below equation holds.
\begin{align*}
	y_1 - \frac{1}{T} \left( \iota_T\iota_T^{'}\right) y_1 = y_1 - \frac{1}{T} \iota_T \left( \iota_T^{'} y_1 \right) = y_1 - \bar{y_1} \iota_T
\end{align*}
This is true for all individuals, i.e. $\forall i\ \in\ \left\{ 1,2,\cdots,N \right\}$. So we are done.

\end{document}
























